{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Result</th>\n",
       "      <th>Hour</th>\n",
       "      <th>WhiteElo</th>\n",
       "      <th>BlackElo</th>\n",
       "      <th>Opening</th>\n",
       "      <th>TimeControl</th>\n",
       "      <th>Termination</th>\n",
       "      <th>...</th>\n",
       "      <th>862</th>\n",
       "      <th>863</th>\n",
       "      <th>864</th>\n",
       "      <th>865</th>\n",
       "      <th>866</th>\n",
       "      <th>867</th>\n",
       "      <th>868</th>\n",
       "      <th>869</th>\n",
       "      <th>870</th>\n",
       "      <th>871</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1512</td>\n",
       "      <td>1570</td>\n",
       "      <td>28072</td>\n",
       "      <td>1074584</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>163</td>\n",
       "      <td>328</td>\n",
       "      <td>75</td>\n",
       "      <td>137</td>\n",
       "      <td>82</td>\n",
       "      <td>77</td>\n",
       "      <td>163</td>\n",
       "      <td>328</td>\n",
       "      <td>81</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>457</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1465</td>\n",
       "      <td>1436</td>\n",
       "      <td>595</td>\n",
       "      <td>1074584</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>459</td>\n",
       "      <td>110</td>\n",
       "      <td>459</td>\n",
       "      <td>310</td>\n",
       "      <td>116</td>\n",
       "      <td>738</td>\n",
       "      <td>167</td>\n",
       "      <td>128</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1665</td>\n",
       "      <td>1765</td>\n",
       "      <td>34752</td>\n",
       "      <td>1074584</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>182</td>\n",
       "      <td>320</td>\n",
       "      <td>648</td>\n",
       "      <td>179</td>\n",
       "      <td>350</td>\n",
       "      <td>168</td>\n",
       "      <td>173</td>\n",
       "      <td>376</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>1377</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1595</td>\n",
       "      <td>1673</td>\n",
       "      <td>11136</td>\n",
       "      <td>47338</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>333</td>\n",
       "      <td>81</td>\n",
       "      <td>105</td>\n",
       "      <td>279</td>\n",
       "      <td>84</td>\n",
       "      <td>101</td>\n",
       "      <td>333</td>\n",
       "      <td>88</td>\n",
       "      <td>105</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>187</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>1511</td>\n",
       "      <td>1621</td>\n",
       "      <td>48705</td>\n",
       "      <td>105090</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>476</td>\n",
       "      <td>81</td>\n",
       "      <td>241</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>191</td>\n",
       "      <td>359</td>\n",
       "      <td>88</td>\n",
       "      <td>241</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 883 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Event  White  Black  Result  Hour  WhiteElo  BlackElo  Opening  \\\n",
       "0      1    123     62       2    22      1512      1570    28072   \n",
       "1      1    457     66       2    22      1465      1436      595   \n",
       "2      0     44    216       2    22      1665      1765    34752   \n",
       "3      2     35   1377       2    22      1595      1673    11136   \n",
       "4      4     44    187       2    22      1511      1621    48705   \n",
       "\n",
       "   TimeControl  Termination  ...  862  863  864  865  866  867  868  869  870  \\\n",
       "0      1074584            1  ...  163  328   75  137   82   77  163  328   81   \n",
       "1      1074584            1  ...  270  459  110  459  310  116  738  167  128   \n",
       "2      1074584            1  ...   94  182  320  648  179  350  168  173  376   \n",
       "3        47338            1  ...  333   81  105  279   84  101  333   88  105   \n",
       "4       105090            1  ...  476   81  241   90   84  191  359   88  241   \n",
       "\n",
       "   871  \n",
       "0  137  \n",
       "1   99  \n",
       "2  682  \n",
       "3  279  \n",
       "4  451  \n",
       "\n",
       "[5 rows x 883 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importar librería pandas y cargar el archivo csv\n",
    "import pandas as pd\n",
    "path = \"C:/Users/franc/Documents/Lic. IA/Semestre 4/Machine learning\" # Cambiar por la ruta de los datos\n",
    "df = pd.read_csv(path + '/data/chess_games_clean2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 716289 entries, 0 to 716288\n",
      "Columns: 883 entries, Event to 871\n",
      "dtypes: int64(882), object(1)\n",
      "memory usage: 4.7+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['Event', 'White', 'Black', 'Hour', 'WhiteElo', 'BlackElo', 'Opening','TimeControl', 'Termination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de modelos a probar\n",
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(max_depth=10),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando modelos para datos sin \"AN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[columnas]\n",
    "y = df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)\n",
    "\n",
    "#preprocesamiento\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Train: \n",
      "Accuracy: 0.3957534583643817\n",
      "Precision: 0.42305337513689545\n",
      "Recall: 0.3957534583643817\n",
      "F1 Score: 0.3598497754146546\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.393834899272641\n",
      "Precision: 0.42199939790050384\n",
      "Recall: 0.393834899272641\n",
      "F1 Score: 0.35810053907310285\n",
      "\n",
      "RandomForestClassifier\n",
      "Train: \n",
      "Accuracy: 0.4815079812435976\n",
      "Precision: 0.48587781353871945\n",
      "Recall: 0.4815079812435976\n",
      "F1 Score: 0.482307520418671\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.47142218933672114\n",
      "Precision: 0.4762261056065904\n",
      "Recall: 0.47142218933672114\n",
      "F1 Score: 0.4723037256943947\n",
      "\n",
      "LogisticRegression\n",
      "Train: \n",
      "Accuracy: 0.4673429535225843\n",
      "Precision: 0.4665830331883109\n",
      "Recall: 0.4673429535225843\n",
      "F1 Score: 0.4668890892664371\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.46734562816736236\n",
      "Precision: 0.46678021428390215\n",
      "Recall: 0.46734562816736236\n",
      "F1 Score: 0.46702670459289014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(model_name)\n",
    "    print(\"Train: \")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Precision: {precision_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print()\n",
    "    print(\"Test: \")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando modelos solo para AN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Result', 'AN'])\n",
    "X = X.drop(columns = columnas)\n",
    "y = df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Train: \n",
      "Accuracy: 0.6557271770637191\n",
      "Precision: 0.6549195035848006\n",
      "Recall: 0.6557271770637191\n",
      "F1 Score: 0.6545419022613629\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.6568847813036619\n",
      "Precision: 0.6560033259233534\n",
      "Recall: 0.6568847813036619\n",
      "F1 Score: 0.6556772690531881\n",
      "\n",
      "RandomForestClassifier\n",
      "Train: \n",
      "Accuracy: 0.7113384790700678\n",
      "Precision: 0.7146576846652805\n",
      "Recall: 0.7113384790700678\n",
      "F1 Score: 0.7100996436976987\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.7083862681316226\n",
      "Precision: 0.7113775728846848\n",
      "Recall: 0.7083862681316226\n",
      "F1 Score: 0.7071214911095347\n",
      "\n",
      "LogisticRegression\n",
      "Train: \n",
      "Accuracy: 0.6857028677331592\n",
      "Precision: 0.6931755820063924\n",
      "Recall: 0.6857028677331592\n",
      "F1 Score: 0.6842857883601688\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.6850996104929568\n",
      "Precision: 0.6925099576654784\n",
      "Recall: 0.6850996104929568\n",
      "F1 Score: 0.6835907984047258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(model_name)\n",
    "    print(\"Train: \")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Precision: {precision_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print()\n",
    "    print(\"Test: \")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando todas las columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usan sólo los dos modelos que daban los mejores resultados en los anteriores entrenamientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(max_depth=10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Result', 'AN'])\n",
    "y = df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Train: \n",
      "Accuracy: 0.6705117175161553\n",
      "Precision: 0.6770225006217423\n",
      "Recall: 0.6705117175161553\n",
      "F1 Score: 0.6683743535673632\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.6712295299389912\n",
      "Precision: 0.6771808366189367\n",
      "Recall: 0.6712295299389912\n",
      "F1 Score: 0.669054894600069\n",
      "\n",
      "RandomForestClassifier\n",
      "Train: \n",
      "Accuracy: 0.7284841483270539\n",
      "Precision: 0.7298550211369128\n",
      "Recall: 0.7284841483270539\n",
      "F1 Score: 0.7275126224015617\n",
      "\n",
      "Test: \n",
      "Accuracy: 0.7243085900961901\n",
      "Precision: 0.7256159661779851\n",
      "Recall: 0.7243085900961901\n",
      "F1 Score: 0.7233590572909118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    print(model_name)\n",
    "    print(\"Train: \")\n",
    "    print(f\"Accuracy: {accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Precision: {precision_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_train, y_pred_train, average='weighted')}\")\n",
    "    print()\n",
    "    print(\"Test: \")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_test)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred_test, average='weighted')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch para RandomForest (no se terminó de entrenar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X = df.drop(columns=['Result', 'AN'])\\ny = df['Result']\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)\\n\\nscaler = MinMaxScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns=['Result', 'AN'])\n",
    "y = df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=90)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# GridSearchCV para RandomForestClassifier\\nparam_grid = {\\n    'n_estimators': [10, 100],\\n    'max_depth': [10, 100],\\n}\\n\\nmodel = RandomForestClassifier()\\ngrid_search = GridSearchCV(model, param_grid)\\ngrid_search.fit(X_train, y_train)\\nprint(grid_search.best_params_)\\ny_pred = grid_search.predict(X_test)\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV para RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100],\n",
    "    'max_depth': [10, 100],\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(model, param_grid)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "y_pred = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Precision: {precision_score(y_test, y_pred, average=\\'weighted\\')}\")\\nprint(f\"Recall: {recall_score(y_test, y_pred, average=\\'weighted\\')}\")\\nprint(f\"F1 Score: {f1_score(y_test, y_pred, average=\\'weighted\\')}\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "|Modelo| sin AN|con AN| todas|\n",
    "|------|-------|------|------|\n",
    "|MultinomialNB| 0.39|0.65|0.67|\n",
    "|RandomForest|0.48|0.70| 0.72|\n",
    "|Logistic| 0.46|0.68| |\n",
    "|RRN|0.33|0.36| |\n",
    "|LSTM| | | 0.73|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: usando Red neuronal LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red neuronal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.sample(6000)\n",
    "\n",
    "X = df1.drop(columns=['Result', 'AN'])\n",
    "X = X.drop(columns = columnas)\n",
    "y = df1['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 1s/step - accuracy: 0.5174 - loss: 1.0290 - val_accuracy: 0.7294 - val_loss: 0.6490\n",
      "Epoch 2/2\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - accuracy: 0.7708 - loss: 0.5460 - val_accuracy: 0.7383 - val_loss: 0.6203\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 170ms/step - accuracy: 0.7365 - loss: 0.6330\n",
      "Accuracy: 0.7383333444595337\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Crear el modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim= X.max().max() + 1, output_dim=128, input_length=1))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=4, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "optimizer = Adam(learning_rate=0.01) \n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algún momento sí se entrenaba relativamente rápido pero después cada época tardaba en entrenarse 2 horas. El siguiente resultado es de un modelo que se entrenó en media hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 159ms/step\n",
      "Accuracy: 0.7383333333333333\n",
      "Precision: 0.7419374876426651\n",
      "Recall: 0.7383333333333333\n",
      "F1 Score: 0.7382229865320497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#matriz de confusion y metricas\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [np.argmax(pred) for pred in y_pred]\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "Podemos notar en las métricas de evaluación que es difícil para un modelo predecir, a partir de las características de las partidas, cómo resultará el resultado de la partida. Esto debido a que en partidas en línea, los usuarios son demasiado diferentes en cuanto a experiencia, por lo que hay todo tipo de partidas. Otro factor es el tiempo, ya que algunas partidas terminan por el tiempo así que puede terminar abruptamente una partida y no tener un patrón al momento de entrenar el modelo. Otras partidas en cambio pueden durar demasiado tiempo y no ser realmente una partida compleja, por lo que el modelo tampoco puede encontrar ciertos comportamientos. Quizá si se tuviera el dato de en qué tiempo se hace cada jugada, esto podría mejorar el modelo.  \n",
    "Aún cuando se probó una red neuronal LSTM y se realizó una tokenización y un padding de secuencias, el accuracy no fue del todo bueno. Mi interpretación sobre esto es que no necesariamente existe una interpretación sintáctica como tal en las jugadas de cada partida, por lo que no necesariamente el modelo puede encontrar patrones para el entrenamiento. Quizás otra representación para las partidas podría resultar en un mejor resultado para los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
